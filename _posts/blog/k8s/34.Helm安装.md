# k8s-helm的安装和配置

`Helm`这个东西其实早有耳闻，但是一直没有用在生产环境，而且现在对这货的评价也是褒贬不一。正好最近需要再次部署一套测试环境，对于单体服务，部署一套测试环境我相信还是非常快的，但是对于微服务架构的应用，要部署一套新的环境，就有点折磨人了，微服务越多、你就会越绝望的。虽然我们线上和测试环境已经都迁移到了`kubernetes`环境，但是每个微服务也得维护一套`yaml`文件，而且每个环境下的配置文件也不太一样，部署一套新的环境成本是真的很高。如果我们能使用类似于`yum`的工具来安装我们的应用的话是不是就很爽歪歪了啊？`Helm`就相当于`kubernetes`环境下的`yum`包管理工具。

### 用途
做为 Kubernetes 的一个包管理工具，`Helm`具有如下功能：

- 创建新的 chart
- chart 打包成 tgz 格式
- 上传 chart 到 chart 仓库或从仓库中下载 chart
- 在`Kubernetes`集群中安装或卸载 chart
- 管理用`Helm`安装的 chart 的发布周期

### 重要概念

Helm 有三个重要概念：

- chart：包含了创建`Kubernetes`的一个应用实例的必要信息
- config：包含了应用发布配置信息
- release：是一个 chart 及其配置的一个运行实例

### Helm组件

Helm 有以下两个组成部分：
![image.png](https://cdn.nlark.com/yuque/0/2019/png/372898/1566441262246-f2586049-ba5b-4534-8769-bd5131bf0217.png#align=left&display=inline&height=270&name=image.png&originHeight=359&originWidth=638&size=113386&status=done&width=479)

`Helm Client` 是用户命令行工具，其主要负责如下：

- 本地 chart 开发
- 仓库管理
- 与 Tiller sever 交互
- 发送预安装的 chart
- 查询 release 信息
- 要求升级或卸载已存在的 release

`Tiller Server`是一个部署在`Kubernetes`集群内部的 server，其与 Helm client、Kubernetes API server 进行交互。Tiller server 主要负责如下：
- 监听来自 Helm client 的请求
- 通过 chart 及其配置构建一次发布
- 安装 chart 到`Kubernetes`集群，并跟踪随后的发布
- 通过与`Kubernetes`交互升级或卸载 chart
- 简单的说，client 管理 charts，而 server 管理发布 release

helm的下载安装：
```shell
wget https://get.helm.sh/helm-v2.14.3-linux-amd64.tar.gz
tar -xf helm-v2.14.3-linux-amd64.tar.gz
mv linux-amd64/helm /usr/local/bin/helm

# helm version
Client: &version.Version{SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"}
Error: could not find tiller
```


tiller服务端的初始化，注意版本号
```shell
helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.14.3 \
--stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts

# helm version
Client: &version.Version{SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"}
Server: &version.Version{SemVer:"v2.14.3", GitCommit:"0e7f3b6637f7af8fcfddb3d2941fcc7cbebb0085", GitTreeState:"clean"}
```

Helm 服务端正常安装完成后，`Tiller`默认被部署在`kubernetes`集群的`kube-system`命名空间下：
```shell
# kubectl get pod -n kube-system -l app=helm
NAME                             READY   STATUS    RESTARTS   AGE
tiller-deploy-6867df9fc6-h7pbg   1/1     Running   0          5m9s
```

安装成功后，您可以使用以下命令管理Helm：

- 执行以下命令，查看在存储库中可用的所有 Helm charts：

```
helm search
```

- 执行以下命令，更新charts列表：

```
helm repo update
```

- 执行以下命令，查看在群集上安装的Charts列表：

```
helm ls
```

另外一个值得注意的问题是`RBAC`，我们的 kubernetes 集群是1.10.0版本的，默认开启了`RBAC`访问控制，所以我们需要为`Tiller`创建一个`ServiceAccount`，让他拥有执行的权限，详细内容可以查看 Helm 文档中的[Role-based Access Control](https://docs.helm.sh/using_helm/#role-based-access-control)。
创建`rbac.yaml`文件：

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
```

然后使用`kubectl`创建：

```shell
$ kubectl create -f rbac-config.yaml
serviceaccount "tiller" created
clusterrolebinding.rbac.authorization.k8s.io "tiller" created
```

创建了`tiller`的 ServceAccount 后还没完，因为我们的 Tiller 之前已经就部署成功了，而且是没有指定 ServiceAccount 的，所以我们需要给 Tiller 打上一个 ServiceAccount 的补丁：

```shell
$ kubectl patch deploy --namespace kube-system tiller-deploy -p '{"spec":{"template":{"spec":{"serviceAccount":"tiller"}}}}'
```

> 上面这一步非常重要，不然后面在使用 Helm 的过程中可能出现`Error: no available release name found`的错误信息。
> 至此, `Helm`客户端和服务端都配置完成了，接下来我们看看如何使用吧。


### 使用

我们现在了尝试创建一个 Chart：

```shell
$ helm create hello-helm
Creating hello-helm
$ tree hello-helm
hello-helm
├── charts
├── Chart.yaml
├── templates
│   ├── deployment.yaml
│   ├── _helpers.tpl
│   ├── ingress.yaml
│   ├── NOTES.txt
│   └── service.yaml
└── values.yaml
2 directories, 7 files
```

我们通过查看`templates`目录下面的`deployment.yaml`文件可以看出默认创建的 Chart 是一个 nginx 服务，具体的每个文件是干什么用的，我们可以前往 [Helm 官方文档](https://docs.helm.sh/developing_charts/#charts)进行查看，后面会和大家详细讲解的。比如这里我们来安装 1.7.9 这个版本的 nginx，则我们更改 value.yaml 文件下面的 image tag 即可，将默认的 stable 更改为 1.7.9，为了测试方便，我们把 Service 的类型也改成 NodePort

```yaml
...
image:
  repository: nginx
  tag: 1.7.9
  pullPolicy: IfNotPresent
nameOverride: ""
fullnameOverride: ""
service:
  type: NodePort
  port: 80
...
```

现在我们来尝试安装下这个 Chart :

```shell
# helm install ./hello-helm
NAME:   washing-ragdoll
LAST DEPLOYED: Thu Aug 22 11:25:22 2019
NAMESPACE: default
STATUS: DEPLOYED
RESOURCES:
==> v1/Deployment
NAME                        READY  UP-TO-DATE  AVAILABLE  AGE
washing-ragdoll-hello-helm  0/1    1           0          0s
==> v1/Pod(related)
NAME                                         READY  STATUS             RESTARTS  AGE
washing-ragdoll-hello-helm-75dbcf4695-drq7l  0/1    ContainerCreating  0         0s
==> v1/Service
NAME                        TYPE      CLUSTER-IP     EXTERNAL-IP  PORT(S)       AGE
washing-ragdoll-hello-helm  NodePort  10.111.185.26  <none>       80:30621/TCP  0s
NOTES:
1. Get the application URL by running these commands:
  export NODE_PORT=$(kubectl get --namespace default -o jsonpath="{.spec.ports[0].nodePort}" services washing-ragdoll-hello-helm)
  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath="{.items[0].status.addresses[0].address}")
  echo http://$NODE_IP:$NODE_PORT

$ kubectl get pods -l app=hello-helm
NAME                                      READY     STATUS    RESTARTS   AGE
iced-ferret-hello-helm-58cb69d5bb-s9f2m   1/1       Running   0          2m
$ kubectl get svc -l app=hello-helm
NAME                       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
iced-ferret-hello-helm   NodePort   10.104.127.141   <none>        80:31236/TCP   3m
```

等到 Pod 创建完成后，我们可以根据创建的 Service 的 NodePort 来访问该服务了，然后在浏览器中打开`http://k8s.haimaxy.com:31236`就可以正常的访问我们刚刚部署的 nginx 应用了。

查看`release`：

```shell
$ helm list
NAME          REVISION  UPDATED                   STATUS    CHART             APP VERSION NAMESPACE
winning-zebra 1         Thu Aug 30 23:50:29 2018  DEPLOYED  hello-helm-0.1.0  1.0         default
```

打包`chart`：

```shell
$ helm package hello-helm
Successfully packaged chart and saved it to: /root/course/kubeadm/helm/hello-helm-0.1.0.tgz
```

然后我们就可以将打包的`tgz`文件分发到任意的服务器上，通过`helm fetch`就可以获取到该 Chart 了。
删除`release`：

```shell
$ helm delete winning-zebra
release "winning-zebra" deleted
```

然后我们看到`kubernetes`集群上的该 nginx 服务也已经被删除了。

```shell
$ kubectl get pods -l app=hello-helm
No resources found.
```





